"""
Comprehensive test suite for Kafka Inventory Consumer race condition debugging.

These tests verify:
1. Race condition detection with concurrent updates
2. Idempotency of message processing
3. Atomic database transaction handling
4. No data corruption under high load
5. Proper Kafka offset management
6. Inventory consistency across concurrent partitions
"""

import pytest
import os
import sys
import subprocess
import time
import json
import psycopg2
import logging
from kafka import KafkaProducer, KafkaConsumer
from kafka.errors import KafkaError
from psycopg2 import sql
from concurrent.futures import ThreadPoolExecutor
import threading
import random

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Test configuration
KAFKA_BROKERS = os.getenv("KAFKA_BROKERS", "localhost:9092")
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME", "inventory_test")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD", "postgres")

DATABASE_URL = f"postgres://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"


@pytest.fixture(scope="session")
def db_connection():
    """Create a database connection for the test session."""
    conn = psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database="postgres"  # Connect to default DB first
    )
    conn.autocommit = True
    cursor = conn.cursor()

    # Drop and recreate test database
    try:
        cursor.execute(f"DROP DATABASE IF EXISTS {DB_NAME}")
    except psycopg2.Error as e:
        logger.warning(f"Could not drop database: {e}")

    cursor.execute(f"CREATE DATABASE {DB_NAME}")
    cursor.close()
    conn.close()

    # Connect to test database
    test_conn = psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME
    )

    yield test_conn
    test_conn.close()


@pytest.fixture(autouse=True)
def setup_database(db_connection):
    """Set up the database schema before each test."""
    cursor = db_connection.cursor()

    # Create tables
    cursor.execute("""
    CREATE TABLE IF NOT EXISTS products (
        id SERIAL PRIMARY KEY,
        sku VARCHAR(100) UNIQUE NOT NULL,
        current_stock INTEGER NOT NULL DEFAULT 0,
        version INTEGER NOT NULL DEFAULT 0,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        last_event_id VARCHAR(255)
    )
    """)

    cursor.execute("""
    CREATE TABLE IF NOT EXISTS processed_events (
        event_id VARCHAR(255) PRIMARY KEY,
        product_id VARCHAR(100) NOT NULL,
        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)

    db_connection.commit()
    yield

    # Clean up after test
    cursor.execute("DROP TABLE IF EXISTS processed_events")
    cursor.execute("DROP TABLE IF EXISTS products")
    db_connection.commit()
    cursor.close()


def get_db_connection():
    """Create a new database connection."""
    return psycopg2.connect(
        host=DB_HOST,
        port=DB_PORT,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME
    )


def insert_product(sku, stock=100):
    """Insert a product into the database."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO products (sku, current_stock, version) VALUES (%s, %s, 0)",
        (sku, stock)
    )
    conn.commit()
    cursor.close()
    conn.close()


def get_product_stock(sku):
    """Get the current stock for a product."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT current_stock FROM products WHERE sku = %s", (sku,))
    result = cursor.fetchone()
    cursor.close()
    conn.close()
    return result[0] if result else None


def get_processed_events_count():
    """Get the count of processed events."""
    conn = get_db_connection()
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM processed_events")
    count = cursor.fetchone()[0]
    cursor.close()
    conn.close()
    return count


class TestRaceConditionDetection:
    """Tests for detecting race conditions in concurrent updates."""

    def test_no_race_with_sequential_updates(self, db_connection):
        """
        Baseline test: Sequential updates should always result in correct inventory.
        This test should PASS after the fix.
        """
        sku = "TEST_SEQ_001"
        insert_product(sku, stock=100)

        # Sequential updates
        conn = get_db_connection()
        cursor = conn.cursor()

        # Update 1: +10
        cursor.execute(
            "UPDATE products SET current_stock = current_stock + 10 WHERE sku = %s",
            (sku,)
        )
        conn.commit()

        # Update 2: -5
        cursor.execute(
            "UPDATE products SET current_stock = current_stock - 5 WHERE sku = %s",
            (sku,)
        )
        conn.commit()

        cursor.close()
        conn.close()

        # Final stock should be 105
        final_stock = get_product_stock(sku)
        assert final_stock == 105, f"Expected 105, got {final_stock}"

    def test_concurrent_updates_consistency(self, db_connection):
        """
        TEST FAILS with buggy code: Concurrent read-modify-write operations
        cause lost updates and data inconsistency.

        This test simulates the classic lost update race condition.
        """
        sku = "TEST_RACE_001"
        insert_product(sku, stock=100)

        def simulate_racy_update(delta):
            """Simulate a racy read-modify-write pattern."""
            conn = get_db_connection()
            cursor = conn.cursor()

            # Simulate delay to increase race window
            time.sleep(random.uniform(0.001, 0.01))

            # Read current stock (RACE: Another thread might update between read and write)
            cursor.execute("SELECT current_stock FROM products WHERE sku = %s", (sku,))
            current_stock = cursor.fetchone()[0]

            # Simulate processing time
            time.sleep(random.uniform(0.001, 0.01))

            # Write new stock (RACE: This might overwrite another thread's update)
            new_stock = current_stock + delta
            cursor.execute(
                "UPDATE products SET current_stock = %s WHERE sku = %s",
                (new_stock, sku)
            )
            conn.commit()
            cursor.close()
            conn.close()

        # Execute concurrent racy updates
        updates = [10, -5, 8, -3, 15, -7, 12, -4]
        expected_final = 100 + sum(updates)  # 126

        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(simulate_racy_update, delta) for delta in updates]
            for future in futures:
                future.result()

        final_stock = get_product_stock(sku)

        # This assertion FAILS with buggy code (due to lost updates)
        # and PASSES with proper locking
        assert final_stock == expected_final, \
            f"Lost updates detected: expected {expected_final}, got {final_stock}"

    def test_concurrent_partition_simulation(self, db_connection):
        """
        TEST FAILS with buggy code: Simulates multiple Kafka partitions
        writing to the same inventory item concurrently.

        Each partition processes messages independently, causing race conditions.
        """
        sku = "TEST_PARTITION_001"
        insert_product(sku, stock=1000)

        def partition_worker(partition_id, num_messages):
            """Simulate a Kafka partition worker processing messages."""
            for i in range(num_messages):
                delta = random.randint(-10, 10)

                conn = get_db_connection()
                cursor = conn.cursor()

                # Simulate message processing delay
                time.sleep(random.uniform(0.0001, 0.001))

                # Read-modify-write (RACE CONDITION!)
                cursor.execute("SELECT current_stock FROM products WHERE sku = %s", (sku,))
                current = cursor.fetchone()[0]

                cursor.execute(
                    "UPDATE products SET current_stock = %s WHERE sku = %s",
                    (current + delta, sku)
                )
                conn.commit()
                cursor.close()
                conn.close()

        # Simulate 4 Kafka partitions processing messages concurrently
        num_partitions = 4
        messages_per_partition = 25

        with ThreadPoolExecutor(max_workers=num_partitions) as executor:
            futures = [
                executor.submit(partition_worker, p, messages_per_partition)
                for p in range(num_partitions)
            ]
            for future in futures:
                future.result()

        final_stock = get_product_stock(sku)

        # Stock should never go negative
        assert final_stock >= 0, f"Stock went negative: {final_stock}"

        # With proper locking, the final stock should be deterministic
        # (Without locking, we get random lost updates)


class TestIdempotency:
    """Tests for idempotent message processing."""

    def test_duplicate_event_not_double_processed(self, db_connection):
        """
        TEST FAILS with buggy code: Duplicate events are processed twice,
        applying the same inventory change twice.

        With proper idempotency, duplicate events should be detected and skipped.
        """
        sku = "TEST_IDEMPOTENT_001"
        event_id = "EVENT_001"
        insert_product(sku, stock=100)

        conn = get_db_connection()
        cursor = conn.cursor()

        # Process event first time
        cursor.execute(
            "UPDATE products SET current_stock = current_stock + 10 WHERE sku = %s",
            (sku,)
        )
        cursor.execute(
            "INSERT INTO processed_events (event_id, product_id) VALUES (%s, %s)",
            (event_id, sku)
        )
        conn.commit()

        # Try to process the same event again
        # With proper idempotency check, this should be skipped
        cursor.execute(
            "SELECT COUNT(*) FROM processed_events WHERE event_id = %s",
            (event_id,)
        )
        if cursor.fetchone()[0] == 0:
            # Event not processed yet
            cursor.execute(
                "UPDATE products SET current_stock = current_stock + 10 WHERE sku = %s",
                (sku,)
            )
            cursor.execute(
                "INSERT INTO processed_events (event_id, product_id) VALUES (%s, %s)",
                (event_id, sku)
            )
            conn.commit()
        else:
            # Event already processed, skip
            pass

        cursor.close()
        conn.close()

        final_stock = get_product_stock(sku)

        # Stock should be 110 (only processed once), not 120
        assert final_stock == 110, \
            f"Duplicate processing detected: expected 110, got {final_stock}"

    def test_concurrent_duplicate_handling(self, db_connection):
        """
        TEST FAILS with buggy code: Concurrent threads processing the same
        duplicate event can cause it to be processed multiple times.
        """
        sku = "TEST_DUP_CONCURRENT_001"
        event_id = "DUP_EVENT_001"
        insert_product(sku, stock=100)

        def try_process_event():
            """Try to process an event (simulating concurrent consumption)."""
            conn = get_db_connection()
            cursor = conn.cursor()

            try:
                # Start transaction
                cursor.execute("BEGIN")

                # Check if already processed
                cursor.execute(
                    "SELECT COUNT(*) FROM processed_events WHERE event_id = %s FOR UPDATE",
                    (event_id,)
                )
                if cursor.fetchone()[0] > 0:
                    conn.rollback()
                    return False

                # Process event
                cursor.execute(
                    "UPDATE products SET current_stock = current_stock + 5 WHERE sku = %s",
                    (sku,)
                )
                cursor.execute(
                    "INSERT INTO processed_events (event_id, product_id) VALUES (%s, %s)",
                    (event_id, sku)
                )
                cursor.execute("COMMIT")
                conn.commit()
                return True
            except Exception as e:
                conn.rollback()
                return False
            finally:
                cursor.close()
                conn.close()

        # Simulate 5 concurrent threads trying to process the same event
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = [executor.submit(try_process_event) for _ in range(5)]
            results = [f.result() for f in futures]

        # Only ONE thread should have successfully processed it
        assert sum(results) == 1, \
            f"Expected only 1 successful processing, got {sum(results)}"

        final_stock = get_product_stock(sku)

        # Stock should be 105 (only incremented once)
        assert final_stock == 105, \
            f"Duplicate concurrent processing detected: expected 105, got {final_stock}"


class TestTransactionIsolation:
    """Tests for proper database transaction isolation."""

    def test_serializable_isolation_prevents_lost_updates(self, db_connection):
        """
        TEST FAILS with buggy code: Using the wrong isolation level allows
        lost updates even with row locks.

        SERIALIZABLE isolation should prevent lost updates.
        """
        sku = "TEST_ISOLATION_001"
        insert_product(sku, stock=100)

        results = []
        lock = threading.Lock()

        def transaction_with_isolation(delta, isolation_level):
            """Execute a transaction with specific isolation level."""
            try:
                conn = get_db_connection()
                # Set isolation level (SERIALIZABLE prevents most race conditions)
                conn.set_isolation_level(isolation_level)
                cursor = conn.cursor()

                # Read current stock
                cursor.execute("SELECT current_stock FROM products WHERE sku = %s", (sku,))
                current = cursor.fetchone()[0]

                # Simulate processing delay to maximize race window
                time.sleep(0.005)

                # Write new stock
                cursor.execute(
                    "UPDATE products SET current_stock = %s WHERE sku = %s",
                    (current + delta, sku)
                )
                conn.commit()
                cursor.close()
                conn.close()

                with lock:
                    results.append((delta, True))
            except psycopg2.errors.SerializationFailure:
                # Serialization failed - this is expected with SERIALIZABLE isolation
                with lock:
                    results.append((delta, False))
            except Exception as e:
                logger.error(f"Transaction failed: {e}")
                with lock:
                    results.append((delta, False))

        # Run two concurrent transactions with SERIALIZABLE isolation
        import psycopg2.extensions

        with ThreadPoolExecutor(max_workers=2) as executor:
            futures = [
                executor.submit(transaction_with_isolation, 10,
                               psycopg2.extensions.ISOLATION_LEVEL_SERIALIZABLE),
                executor.submit(transaction_with_isolation, -5,
                               psycopg2.extensions.ISOLATION_LEVEL_SERIALIZABLE)
            ]
            for future in futures:
                future.result()

        # With SERIALIZABLE isolation, at least one should succeed
        successful = sum(1 for _, succeeded in results if succeeded)
        assert successful >= 1, "No transactions succeeded with SERIALIZABLE isolation"


class TestLockingMechanisms:
    """Tests for proper mutex and lock usage."""

    def test_per_product_locking_prevents_hotspot_contention(self, db_connection):
        """
        TEST: With per-product locking (instead of global lock),
        different products should not block each other.

        This test verifies that locking is fine-grained enough.
        """
        # Create multiple products
        skus = ["TEST_LOCK_001", "TEST_LOCK_002", "TEST_LOCK_003", "TEST_LOCK_004"]
        for sku in skus:
            insert_product(sku, stock=100)

        execution_times = {}

        def process_product(sku, operations=50):
            """Process many updates for a single product."""
            start = time.time()

            for _ in range(operations):
                conn = get_db_connection()
                cursor = conn.cursor()

                cursor.execute(
                    "UPDATE products SET current_stock = current_stock + 1 WHERE sku = %s",
                    (sku,)
                )
                conn.commit()
                cursor.close()
                conn.close()

            elapsed = time.time() - start
            execution_times[sku] = elapsed

        # Process multiple products concurrently
        # With fine-grained locking, they should complete in similar time
        # With a global lock, one thread will serialize everything
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(process_product, sku, 50) for sku in skus]
            for future in futures:
                future.result()

        # Verify all products were updated
        for sku in skus:
            stock = get_product_stock(sku)
            assert stock == 150, f"{sku} should have 150, got {stock}"

        # With per-product locking, all times should be similar
        times = list(execution_times.values())
        avg_time = sum(times) / len(times)

        # Check that no single product took excessively longer (indicating lock contention)
        for sku, elapsed in execution_times.items():
            # Allow 2x variance due to system scheduling
            assert elapsed < avg_time * 2.5, \
                f"{sku} took {elapsed}s vs avg {avg_time}s - suggests global lock contention"


class TestAtomicOperations:
    """Tests for atomic update operations."""

    def test_version_based_optimistic_locking(self, db_connection):
        """
        TEST FAILS with buggy code: Without version numbers,
        concurrent updates can silently overwrite each other.

        Version-based locking ensures only forward progress.
        """
        sku = "TEST_VERSION_001"
        insert_product(sku, stock=100)

        successful_updates = []
        lock = threading.Lock()

        def versioned_update(delta):
            """Try to update with version check."""
            conn = get_db_connection()
            cursor = conn.cursor()

            # Read current state with version
            cursor.execute(
                "SELECT current_stock, version FROM products WHERE sku = %s FOR UPDATE",
                (sku,)
            )
            current_stock, current_version = cursor.fetchone()

            # Simulate processing
            time.sleep(random.uniform(0.001, 0.005))

            # Try to update only if version matches (optimistic locking)
            cursor.execute("""
                UPDATE products
                SET current_stock = %s, version = %s
                WHERE sku = %s AND version = %s
            """, (current_stock + delta, current_version + 1, sku, current_version))

            rows_updated = cursor.rowcount
            conn.commit()
            cursor.close()
            conn.close()

            with lock:
                successful_updates.append(rows_updated > 0)

            return rows_updated > 0

        # Try concurrent updates with version checking
        deltas = [10, -5, 8, -3, 15]
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [executor.submit(versioned_update, delta) for delta in deltas]
            for future in futures:
                future.result()

        # All updates should succeed (they serialize due to version checks)
        assert len(successful_updates) == len(deltas), \
            f"Version checks not working: {successful_updates}"

        # Final stock should equal initial + sum of deltas
        final_stock = get_product_stock(sku)
        expected = 100 + sum(deltas)
        assert final_stock == expected, \
            f"Expected {expected}, got {final_stock}"


class TestConsumerBehavior:
    """Tests for Kafka consumer behavior and offset management."""

    def test_offset_not_committed_before_db_write(self, db_connection):
        """
        TEST FAILS with buggy code: If offset is committed before DB write,
        a crash between the two results in duplicate processing.

        Offsets must be committed AFTER successful database write.
        """
        # This is more of an integration test that requires running the actual service
        # For now, we verify the pattern through database checks

        sku = "TEST_OFFSET_001"
        insert_product(sku, stock=100)

        # Verify deduplication works if we replay the same event
        event_id = "OFFSET_TEST_EVENT"

        conn = get_db_connection()
        cursor = conn.cursor()

        # First processing
        cursor.execute(
            "INSERT INTO processed_events (event_id, product_id) VALUES (%s, %s)",
            (event_id, sku)
        )
        cursor.execute(
            "UPDATE products SET current_stock = current_stock + 10 WHERE sku = %s",
            (sku,)
        )
        conn.commit()

        # Simulate crash and replay - try to process same event again
        # Should detect it was already processed
        cursor.execute(
            "SELECT COUNT(*) FROM processed_events WHERE event_id = %s",
            (event_id,)
        )
        already_processed = cursor.fetchone()[0] > 0

        assert already_processed, "Deduplication not working - event should be marked"

        # Stock should only be incremented once
        cursor.execute("SELECT current_stock FROM products WHERE sku = %s", (sku,))
        stock = cursor.fetchone()[0]

        assert stock == 110, f"Expected 110, got {stock} - event processed twice"

        cursor.close()
        conn.close()


class TestHighLoadConsistency:
    """Tests for data consistency under high load."""

    def test_high_concurrent_load_no_corruption(self, db_connection):
        """
        TEST FAILS with buggy code: Under high concurrent load,
        data corruption and lost updates become evident.

        This stress test creates significant race condition pressure.
        """
        sku = "TEST_LOAD_001"
        initial_stock = 10000
        insert_product(sku, stock=initial_stock)

        def stress_update(operation_id, num_ops=100):
            """Perform many concurrent updates."""
            for _ in range(num_ops):
                conn = get_db_connection()
                cursor = conn.cursor()

                delta = random.choice([1, -1])
                cursor.execute(
                    "UPDATE products SET current_stock = current_stock + %s WHERE sku = %s",
                    (delta, sku)
                )
                conn.commit()
                cursor.close()
                conn.close()

        # Hammer with concurrent workers
        num_workers = 10
        operations_per_worker = 50
        expected_total_ops = num_workers * operations_per_worker

        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [
                executor.submit(stress_update, i, operations_per_worker)
                for i in range(num_workers)
            ]
            for future in futures:
                future.result()

        final_stock = get_product_stock(sku)

        # Stock should never go below 0
        assert final_stock >= 0, f"Negative stock detected: {final_stock}"

        # Stock should be within valid range (initial Â± max possible ops)
        min_possible = initial_stock - expected_total_ops
        max_possible = initial_stock + expected_total_ops

        assert min_possible <= final_stock <= max_possible, \
            f"Stock {final_stock} outside valid range [{min_possible}, {max_possible}]"


class TestGoRaceDetector:
    """Tests that verify race condition can be caught by Go race detector."""

    def test_compile_with_race_detector(self):
        """
        TEST FAILS with buggy code: Running with -race flag should detect the race condition.

        go run -race main.go should report data race warnings.
        """
        # This test verifies that the code can be compiled with race detector
        # The actual race detection would happen at runtime
        test_dir = os.path.dirname(os.path.abspath(__file__))
        main_file = os.path.join(test_dir, "../files/main.go")

        # Verify the main.go file exists
        assert os.path.exists(main_file), f"main.go not found at {main_file}"


if __name__ == "__main__":
    # Run tests with pytest
    pytest.main([__file__, "-v", "-s"])
