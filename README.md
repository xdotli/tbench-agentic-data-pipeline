# ğŸ¤– Terminal Bench Agentic Data Pipeline

**TL;DR:** A multi-agent system with 20+ Claude Code instances working in parallel to generate high-quality training data for terminal-based coding tasks, with focus on **backend software engineering** scenarios.

**Primary Focus: Backend Software Engineering**

This pipeline generates repository-level, realistic backend engineering tasks including:
- API development (REST, GraphQL, WebSocket)
- Database operations, migrations, and query optimization
- Debugging and refactoring in complex codebases
- Testing infrastructure (unit, integration, end-to-end)
- Real-world developer scenarios targeting ~20% pass rate on SOTA models

**Why build an agentic data pipeline?**

Because generating quality training data for coding agents requires creativity, validation, and scaleâ€”tasks perfectly suited for AI collaboration.

## ğŸ¬ See It In Action

Watch 20+ agents working in parallel to generate training data at scale:

### 20x agents in parallel
https://github.com/user-attachments/assets/6cf8819a-343a-4e03-9198-4979d4d160e5

### 6x agents in parallel (easier to follow!)

https://github.com/user-attachments/assets/450cc73b-5316-46ea-a091-278690f4689d


![Pipeline Overview](./readme_images/pipeline-overview.png)

---

## ğŸ“š Table of Contents

- [See It In Action](#-see-it-in-action)
- [High-Level Architecture](#-high-level-architecture)
- [Pipeline Results](#-pipeline-results)
- [Agent Roles & Workflow](#-agent-roles--workflow)
  - [Idea Generation Agents](#-idea-generation-agents)
  - [Datapoint Builder Agents](#-datapoint-builder-agents)
  - [Quality Review Agents](#-quality-review-agents)
- [Task Manager System](#-task-manager-system)
- [Datapoint Structure](#-datapoint-structure)
- [Validation Pipeline](#-validation-pipeline)
- [Infrastructure & Tools](#-infrastructure--tools)
- [Getting Started](#-getting-started)
- [Key Design Decisions](#-key-design-decisions)

---

## ğŸ—ï¸ High-Level Architecture

The pipeline transforms Terminal Bench evaluation tasks into diverse training datapoints through three specialized agent stages:

1. **Seed tasks** from Terminal Bench â†’ **Idea Agents** â†’ Creative variations
2. **Draft ideas** â†’ **Builder Agents** â†’ Complete executable datapoints  
3. **Built datapoints** â†’ **Review Agents** â†’ Production-ready training data

All agents work independently in parallel, coordinated by a central Task Manager that prevents duplication and handles failures.

---

## ğŸ“ˆ Pipeline Configuration

### Backend Engineering Focus
- **Target Difficulty**: ~20% pass rate on SOTA models
- **Languages**: Python, JavaScript, TypeScript
- **Frameworks**: FastAPI, Flask, Django, Express.js, NestJS
- **Databases**: PostgreSQL, MySQL, MongoDB, Redis
- **Task Complexity**: Repository-level, multi-file codebases
- **Scenario Types**: Real-world developer tasks (not toy projects)

### Supported Categories

| Category | Description |
|----------|-------------|
| API Development | REST APIs, GraphQL, WebSocket servers, middleware |
| Backend Engineering | Microservices, server-side processing, concurrency |
| Database Engineering | Migrations, schema design, query optimization |
| Code Refactoring | Legacy code improvement, architecture changes |
| Debugging | Multi-file bugs, race conditions, performance issues |
| Integration Testing | Test infrastructure, mocking, fixtures |
| Microservices | Multi-service architectures, service communication |
| Security | Authentication, authorization, input validation |
| Software Engineering | General engineering practices, design patterns |
| Testing Infrastructure | Unit, integration, E2E test workflows |
| Web Development | Full-stack features, server-side rendering |

### Backend Technology Stack
- **Python**: FastAPI, Flask, Django, SQLAlchemy, Pytest
- **JavaScript/TypeScript**: Express.js, NestJS, Node.js, Jest, Mocha
- **Databases**: PostgreSQL, MySQL, MongoDB, Redis, ORMs
- **Testing**: Pytest, Jest, Mocha, integration test frameworks
- **DevOps**: Docker containers, CI/CD workflows

---

## ğŸ”„ Agent Roles & Workflow

![Single DP Journey](./readme_images/single-dp-journey.png)

Each agent type operates independently with specialized tools and clear responsibilities:

### ğŸ¨ Idea Generation Agents
- **Input**: Seed tasks from Terminal Bench
- **Process**: Analyze core skills â†’ Generate nÃ—multiplier variations â†’ Select best ideas
- **Output**: Draft specifications in shared workspace

**Key Innovation**: Refinement criteria provided only AFTER brainstorming to maximize creativity

### ğŸ”¨ Datapoint Builder Agents  
- **Input**: Draft specifications from idea agents
- **Process**: Build complete scenarios â†’ Validate via software script â†’ Iterate until passing
- **Output**: Executable datapoints with all required components

**Validation Requirements**:
- âœ… Dockerfile builds successfully
- âœ… Tests fail before agent intervention
- âœ… All dependencies present
- âœ… Test weights sum to 1.0

### ğŸ” Quality Review Agents
- **Input**: Validated datapoints from builders
- **Process**: Check quality standards â†’ Edit if needed â†’ Re-validate â†’ Categorize
- **Output**: Approved datapoints with metadata or rejection with reasons

---

## ğŸ“Š Task Manager System

The Task Manager enables parallel agent coordination without complex handoffs:

```python
# Agent claims work atomically
task = tm.get_next_task("idea-agent-07", task_types=["generate_idea"])

# Process independently
draft_ideas = generate_creative_variations(task["seed_data"])

# Complete with results
tm.complete_task(task["id"], "idea-agent-07", {"drafts": draft_ideas})
```

**Features**:
- Atomic task claiming (no collisions)
- Automatic timeout recovery
- Parent-child task tracking
- Real-time status monitoring

---

## ğŸ“ Datapoint Structure

Each training datapoint contains:

```
draft_001_a/
â”œâ”€â”€ prompt.md       # 1-3 sentence task (e.g., "Auth times out with 100+ users. Fix it.")
â”œâ”€â”€ dockerfile      # Ubuntu 24.04 (or similar) environment setup
â”œâ”€â”€ tests.py        # Pytest verification functions
â”œâ”€â”€ weights.json    # Test importance distribution
â””â”€â”€ files/          # Additional resources
    â”œâ”€â”€ app.py      # Broken code to fix
    â””â”€â”€ config.json # Configuration files
```

---

## âœ… Validation Pipeline

Shared validation tools ensure quality across all agents:

1. **Docker Build**: Environment must build successfully
2. **Test Discovery**: Pytest must find all test functions
3. **Fail-First**: Tests must fail in initial state
4. **Dependency Check**: All required packages present
5. **Weight Validation**: Test weights sum to exactly 1.0

---

## ğŸ› ï¸ Infrastructure & Tools

### Workspace Organization
```
agents/
â”œâ”€â”€ idea_agent_workspace/      # Idea generation tools & instructions
â”œâ”€â”€ dp_builder_workspace/      # Building tools & staging areas
â””â”€â”€ review_agent_workspace/    # Review tools & quality checks

shared_workspace/              # Common filesystem for all agents
shared_tools/                  # Validation, patching, utilities
task_manager/                  # Coordination & state management
```

### Agent-Specific Tools
- **Idea Agents**: `get_task_parameters.py`, `get_idea_refinement_details.py`
- **Builder Agents**: `create_dp.py`, `add_dp_to_review.py`
- **Review Agents**: `approve_datapoint.py`, `cancel_datapoint.py`, `show_categories_tags.py`

### Shared Tools
- `validate_datapoint.py` - Complete validation suite
- `patch_dp.py` - Update datapoint components
- `patch_additional_files.py` - Manage resource files

---

## ğŸš€ Getting Started

```bash
# Clone repository
git clone https://github.com/Danau5tin/tbench-agentic-data-pipeline.git
cd tbench-agentic-data-pipeline

# Install dependencies
uv sync

# Initialize seed tasks
python init_seed_tasks.py <path_to_terminal_bench_tasks>

# Launch agents with Claude Code
# Idea Agent:
"See @agents/idea_agent_workspace/workflow_instructions.md - you are the idea generation agent, go!"

# Builder Agent:
"See @agents/dp_builder_workspace/workflow_instructions.md - you are the datapoint builder agent, go!"

# Review Agent:
"See @agents/review_agent_workspace/workflow_instructions.md - you are the quality review agent, go!"
```

---

## ğŸ’¡ Key Design Decisions

### Why Multiple Agent Types?
- **Separation of concerns**: Each agent excels at one task
- **Parallel scaling**: Multiple instances per type
- **Quality gates**: Three-stage validation ensures high standards

### Why Shared Filesystem?
- **Simplicity**: No complex message passing
- **Reliability**: File operations are atomic
- **Debugging**: Easy to inspect intermediate states

### Why Task Manager?
- **Coordination**: Prevents duplicate work
- **Recovery**: Handles agent failures gracefully
- **Monitoring**: Real-time pipeline visibility

---

**Built with Claude Code** ğŸ¤– - This entire multi-agent system was developed using Claude Code, demonstrating the power of AI agents building infrastructure for other AI agents.
